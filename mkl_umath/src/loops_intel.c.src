/*
 * Copyright (c) 2005-2023, NumPy Developers.
 * Copyright (c) 2019-2021, Intel Corporation
 * Copyright 2023 FUJITSU LIMITED
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 *     * Redistributions of source code must retain the above copyright notice,
 *       this list of conditions and the following disclaimer.
 *     * Redistributions in binary form must reproduce the above copyright
 *       notice, this list of conditions and the following disclaimer in the
 *       documentation and/or other materials provided with the distribution.
 *     * Neither the name of Intel Corporation nor the names of its contributors
 *       may be used to endorse or promote products derived from this software
 *       without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/* -*- c -*- */
#include <float.h>
#include <fenv.h>
#include <stdint.h>

#include <arm_sve.h>

#include <sleef.h>

#include "Python.h"

#define NPY_NO_DEPRECATED_API NPY_API_VERSION

#include "numpy/npy_common.h"
#include "numpy/ndarraytypes.h"
#include "numpy/ndarrayobject.h"
#include "numpy/ufuncobject.h"
#include "numpy/npy_math.h"
#include "blocking_utils.h"
#include "loops_intel.h"

/*
 * The following instruction sets are supported by NumPy at this time.
 * - Intel SSE, AVX2, AVX512,
 * - Arm Neon,
 * - IBM VSX(Power) and VX(ZArch).
 * Intrinsic functions for those instruction sets are abstracted in
 * https://github.com/numpy/numpy/tree/maintenance/1.24.x/numpy/core/src/
 * common/simd
 * This `mkl_umath` requires "memory.h", but it is not available for Arm SVE.
 * "memory.h" in https://github.com/numpy/numpy/pull/22265 is used as an
 * alternative.
 */
#include "memory.h"

/* Adapated from NumPy's source code. 
 * https://github.com/numpy/numpy/blob/main/LICENSE.txt */

/*
 * largest simd vector size in bytes numpy supports
 * it is currently a extremely large value as it is only used for memory
 * overlap checks
 */
#ifndef NPY_MAX_SIMD_SIZE
#define NPY_MAX_SIMD_SIZE 1024
#endif

/*
 * cutoff blocksize for pairwise summation
 * decreasing it decreases errors slightly as more pairs are summed but
 * also lowers performance, as the inner loop is unrolled eight times it is
 * effectively 16
 */
#define PW_BLOCKSIZE    128
#define VML_TRANSCEDENTAL_THRESHOLD 0
#define VML_ASM_THRESHOLD 100000
#define VML_D_THRESHOLD 8000

#define MKL_INT_MAX ((npy_intp) ((~((uint32_t) 0)) >> 1))

#define CHUNKED_VML_CALL2(vml_func, n, type, in1, op1)   \
    do {                                                 \
        npy_intp _n_ = (n);                              \
        const npy_intp _chunk_size = MKL_INT_MAX;        \
        type *in1p = (type *) (in1);                     \
        type *op1p = (type *) (op1);                     \
        while (_n_ > _chunk_size) {                      \
            vml_func(in1p, (long*) &_chunk_size, op1p);  \
            _n_ -= _chunk_size;                          \
            in1p += _chunk_size;                         \
            op1p += _chunk_size;                         \
        }                                                \
        if (_n_) {                                       \
            vml_func(in1p, (long*) &_n_, op1p);          \
        }                                                \
    } while (0)



/* for pointers p1, and p2 pointing at contiguous arrays n-elements of size s, are arrays disjoint or same
 *  when these conditions are not met VML functions may product incorrect output
 */
#define DISJOINT_OR_SAME(p1, p2, n, s) (((p1) == (p2)) || ((p2) + (n)*(s) < (p1)) || ((p1) + (n)*(s) < (p2)) )

/*
 * include vectorized functions and dispatchers
 * this file is safe to include also for generic builds
 * platform specific instructions are either masked via the proprocessor or
 * runtime detected
 */

/** Provides the various *_LOOP macros */
#include "fast_loop_macros.h"
#include <stdio.h>


/*
 *****************************************************************************
 **                             FLOAT LOOPS                                 **
 *****************************************************************************
 */



/**begin repeat
 * #sfx = 32, 64#
 * #func_suffix = fx, dx#
 * #type = float, double#
 */
/**begin repeat1
 * #ssrc = contig#
 * #sdst = contig#
 */
/**begin repeat2
 * #math_api = sin, cos, tan, asin, acos, atan,
               sinh, cosh, tanh, asinh, acosh, atanh,
               exp, exp2, expm1,  log, log2, log10, log1p,
               sqrt, cbrt#
 * #error = u10, u10, u10, u10, u10, u10,
            u10, u10, u10, u10, u10, u10,
            u10, u10, u10, u10, u10, u10, u10,
            u05, u10#
 */
static void
@ssrc@_@sdst@_sleef_@math_api@_f@sfx@(npy_intp len, @type@ *src, @type@ *dst,
    const npy_intp ssrc, const npy_intp sdst)
{
    const int vstep = NPY_SIMD_WIDTH / @sfx@;
    npyv_f@sfx@ xa;

    for (; len >= vstep; len -= vstep, src += ssrc*vstep, dst += sdst*vstep) {
        xa = npyv_load_tillz_f@sfx@(src, vstep);
        npyv_f@sfx@ out = Sleef_@math_api@@func_suffix@_@error@sve(xa);
        npyv_store_till_f@sfx@(dst, vstep, out);
    }
    if (len) {
        xa = npyv_load_till_f@sfx@(src, len, 1.);
        npyv_f@sfx@ out = Sleef_@math_api@@func_suffix@_@error@sve(xa);
        npyv_store_till_f@sfx@(dst, len, out);
    }
}
/**end repeat2**/
/**end repeat1**/
/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #c = , d#
 *  #scalarf = f, #
 *  #sfx  = f32, f64#
 */
/**begin repeat1
 * #func = sin, cos, tan, arcsin, arccos, arctan,
           sinh, cosh, tanh, arcsinh, arccosh, arctanh,
           exp, exp2, expm1, log, log2, log10, log1p,
           sqrt, cbrt#
 * #math_api = sin, cos, tan, asin, acos, atan,
               sinh, cosh, tanh, asinh, acosh, atanh,
               exp, exp2, expm1,  log, log2, log10, log1p,
               sqrt, cbrt#
 */
NPY_NO_EXPORT void
@TYPE@_@func@(char **args, const npy_intp *dimensions, const npy_intp *steps, void *NPY_UNUSED(func))
{
    if(DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@))) {
        if(IS_UNARY_CONT(@type@, @type@)) {
            contig_contig_sleef_@math_api@_@sfx@(dimensions[0], (@type@*) args[0],
	        (@type@*) args[1], steps[0]/sizeof(@type@), steps[1]/sizeof(@type@));
            return;
        }
    }
    UNARY_LOOP_DISPATCH(
        DISJOINT_OR_SAME(args[0], args[1], dimensions[0], sizeof(@type@)),
        const @type@ in1 = *(@type@ *)ip1;
        *(@type@ *)op1 = @math_api@@scalarf@(in1);
    )
}

/**end repeat1**/
/**end repeat**/



/*
 *****************************************************************************
 **                              END LOOPS                                  **
 *****************************************************************************
*/
